{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66090e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "128ad3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_file = 'datasets/train.en'\n",
    "kannada_file = 'datasets/train.kn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65719895",
   "metadata": {},
   "source": [
    "# Vocabulary - Alpha Syllabary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce3deaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<START>'\n",
    "PADDING_TOKEN = '<PADDING>'\n",
    "END_TOKEN = '<END>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fae3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "kannada_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ', \n",
    "                      'ँ', 'ఆ', 'ఇ', 'ా', 'ి', 'ీ', 'ు', 'ూ', \n",
    "                      'ಅ', 'ಆ', 'ಇ', 'ಈ', 'ಉ', 'ಊ', 'ಋ', 'ೠ', 'ಌ', 'ಎ', 'ಏ', 'ಐ', 'ಒ', 'ಓ', 'ಔ', \n",
    "                      'ಕ', 'ಖ', 'ಗ', 'ಘ', 'ಙ', \n",
    "                      'ಚ', 'ಛ', 'ಜ', 'ಝ', 'ಞ', \n",
    "                      'ಟ', 'ಠ', 'ಡ', 'ಢ', 'ಣ', \n",
    "                      'ತ', 'ಥ', 'ದ', 'ಧ', 'ನ', \n",
    "                      'ಪ', 'ಫ', 'ಬ', 'ಭ', 'ಮ', \n",
    "                      'ಯ', 'ರ', 'ಱ', 'ಲ', 'ಳ', 'ವ', 'ಶ', 'ಷ', 'ಸ', 'ಹ', \n",
    "                      '಼', 'ಽ', 'ಾ', 'ಿ', 'ೀ', 'ು', 'ೂ', 'ೃ', 'ೄ', 'ೆ', 'ೇ', 'ೈ', 'ೊ', 'ೋ', 'ೌ', '್', 'ೕ', 'ೖ', 'ೞ', 'ೣ', 'ಂ', 'ಃ', \n",
    "                      '೦', '೧', '೨', '೩', '೪', '೫', '೬', '೭', '೮', '೯', PADDING_TOKEN, END_TOKEN]\n",
    "\n",
    "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                        ':', '<', '=', '>', '?', '@', \n",
    "                        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', \n",
    "                        'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', \n",
    "                        'Y', 'Z',\n",
    "                        '[', '\\\\', ']', '^', '_', '`', \n",
    "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
    "                        'y', 'z', \n",
    "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a77a7895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ಕ', 'ನ', '್', 'ನ', 'ಡ'], ['K', 'a', 'n', 'n', 'a', 'd', 'a'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'ಕನ್ನಡ' # ~Kannada\n",
    "text2 = 'Kannada'\n",
    "list(text), list(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "771d9039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ಕಾ', 'KA')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ಕ' + 'ಾ', 'K' + 'A'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877badf",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ddd215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_kannada = {k:v for k, v in enumerate(kannada_vocabulary)}\n",
    "kannada_to_index = {v:k for k, v in enumerate(kannada_vocabulary)}\n",
    "index_to_english = {k:v for k, v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k, v in enumerate(english_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac30bc0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index : Kannada\n",
      "{0: '<START>', 1: ' ', 2: '!', 3: '\"', 4: '#', 5: '$', 6: '%', 7: '&', 8: \"'\", 9: '(', 10: ')', 11: '*', 12: '+', 13: ',', 14: '-', 15: '.', 16: '/', 17: '0', 18: '1', 19: '2', 20: '3', 21: '4', 22: '5', 23: '6', 24: '7', 25: '8', 26: '9', 27: ':', 28: '<', 29: '=', 30: '>', 31: '?', 32: 'ˌ', 33: 'ँ', 34: 'ఆ', 35: 'ఇ', 36: 'ా', 37: 'ి', 38: 'ీ', 39: 'ు', 40: 'ూ', 41: 'ಅ', 42: 'ಆ', 43: 'ಇ', 44: 'ಈ', 45: 'ಉ', 46: 'ಊ', 47: 'ಋ', 48: 'ೠ', 49: 'ಌ', 50: 'ಎ', 51: 'ಏ', 52: 'ಐ', 53: 'ಒ', 54: 'ಓ', 55: 'ಔ', 56: 'ಕ', 57: 'ಖ', 58: 'ಗ', 59: 'ಘ', 60: 'ಙ', 61: 'ಚ', 62: 'ಛ', 63: 'ಜ', 64: 'ಝ', 65: 'ಞ', 66: 'ಟ', 67: 'ಠ', 68: 'ಡ', 69: 'ಢ', 70: 'ಣ', 71: 'ತ', 72: 'ಥ', 73: 'ದ', 74: 'ಧ', 75: 'ನ', 76: 'ಪ', 77: 'ಫ', 78: 'ಬ', 79: 'ಭ', 80: 'ಮ', 81: 'ಯ', 82: 'ರ', 83: 'ಱ', 84: 'ಲ', 85: 'ಳ', 86: 'ವ', 87: 'ಶ', 88: 'ಷ', 89: 'ಸ', 90: 'ಹ', 91: '಼', 92: 'ಽ', 93: 'ಾ', 94: 'ಿ', 95: 'ೀ', 96: 'ು', 97: 'ೂ', 98: 'ೃ', 99: 'ೄ', 100: 'ೆ', 101: 'ೇ', 102: 'ೈ', 103: 'ೊ', 104: 'ೋ', 105: 'ೌ', 106: '್', 107: 'ೕ', 108: 'ೖ', 109: 'ೞ', 110: 'ೣ', 111: 'ಂ', 112: 'ಃ', 113: '೦', 114: '೧', 115: '೨', 116: '೩', 117: '೪', 118: '೫', 119: '೬', 120: '೭', 121: '೮', 122: '೯', 123: '<PADDING>', 124: '<END>'}\n",
      "\n",
      "Kannada : Index\n",
      "{'<START>': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, '<': 28, '=': 29, '>': 30, '?': 31, 'ˌ': 32, 'ँ': 33, 'ఆ': 34, 'ఇ': 35, 'ా': 36, 'ి': 37, 'ీ': 38, 'ు': 39, 'ూ': 40, 'ಅ': 41, 'ಆ': 42, 'ಇ': 43, 'ಈ': 44, 'ಉ': 45, 'ಊ': 46, 'ಋ': 47, 'ೠ': 48, 'ಌ': 49, 'ಎ': 50, 'ಏ': 51, 'ಐ': 52, 'ಒ': 53, 'ಓ': 54, 'ಔ': 55, 'ಕ': 56, 'ಖ': 57, 'ಗ': 58, 'ಘ': 59, 'ಙ': 60, 'ಚ': 61, 'ಛ': 62, 'ಜ': 63, 'ಝ': 64, 'ಞ': 65, 'ಟ': 66, 'ಠ': 67, 'ಡ': 68, 'ಢ': 69, 'ಣ': 70, 'ತ': 71, 'ಥ': 72, 'ದ': 73, 'ಧ': 74, 'ನ': 75, 'ಪ': 76, 'ಫ': 77, 'ಬ': 78, 'ಭ': 79, 'ಮ': 80, 'ಯ': 81, 'ರ': 82, 'ಱ': 83, 'ಲ': 84, 'ಳ': 85, 'ವ': 86, 'ಶ': 87, 'ಷ': 88, 'ಸ': 89, 'ಹ': 90, '಼': 91, 'ಽ': 92, 'ಾ': 93, 'ಿ': 94, 'ೀ': 95, 'ು': 96, 'ೂ': 97, 'ೃ': 98, 'ೄ': 99, 'ೆ': 100, 'ೇ': 101, 'ೈ': 102, 'ೊ': 103, 'ೋ': 104, 'ೌ': 105, '್': 106, 'ೕ': 107, 'ೖ': 108, 'ೞ': 109, 'ೣ': 110, 'ಂ': 111, 'ಃ': 112, '೦': 113, '೧': 114, '೨': 115, '೩': 116, '೪': 117, '೫': 118, '೬': 119, '೭': 120, '೮': 121, '೯': 122, '<PADDING>': 123, '<END>': 124}\n",
      "\n",
      "\n",
      "Index : English\n",
      "{0: '<START>', 1: ' ', 2: '!', 3: '\"', 4: '#', 5: '$', 6: '%', 7: '&', 8: \"'\", 9: '(', 10: ')', 11: '*', 12: '+', 13: ',', 14: '-', 15: '.', 16: '/', 17: '0', 18: '1', 19: '2', 20: '3', 21: '4', 22: '5', 23: '6', 24: '7', 25: '8', 26: '9', 27: ':', 28: '<', 29: '=', 30: '>', 31: '?', 32: '@', 33: 'A', 34: 'B', 35: 'C', 36: 'D', 37: 'E', 38: 'F', 39: 'G', 40: 'H', 41: 'I', 42: 'J', 43: 'K', 44: 'L', 45: 'M', 46: 'N', 47: 'O', 48: 'P', 49: 'Q', 50: 'R', 51: 'S', 52: 'T', 53: 'U', 54: 'V', 55: 'W', 56: 'X', 57: 'Y', 58: 'Z', 59: '[', 60: '\\\\', 61: ']', 62: '^', 63: '_', 64: '`', 65: 'a', 66: 'b', 67: 'c', 68: 'd', 69: 'e', 70: 'f', 71: 'g', 72: 'h', 73: 'i', 74: 'j', 75: 'k', 76: 'l', 77: 'm', 78: 'n', 79: 'o', 80: 'p', 81: 'q', 82: 'r', 83: 's', 84: 't', 85: 'u', 86: 'v', 87: 'w', 88: 'x', 89: 'y', 90: 'z', 91: '{', 92: '|', 93: '}', 94: '~', 95: '<PADDING>', 96: '<END>'}\n",
      "\n",
      "English : Index\n",
      "{'<START>': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, '<': 28, '=': 29, '>': 30, '?': 31, '@': 32, 'A': 33, 'B': 34, 'C': 35, 'D': 36, 'E': 37, 'F': 38, 'G': 39, 'H': 40, 'I': 41, 'J': 42, 'K': 43, 'L': 44, 'M': 45, 'N': 46, 'O': 47, 'P': 48, 'Q': 49, 'R': 50, 'S': 51, 'T': 52, 'U': 53, 'V': 54, 'W': 55, 'X': 56, 'Y': 57, 'Z': 58, '[': 59, '\\\\': 60, ']': 61, '^': 62, '_': 63, '`': 64, 'a': 65, 'b': 66, 'c': 67, 'd': 68, 'e': 69, 'f': 70, 'g': 71, 'h': 72, 'i': 73, 'j': 74, 'k': 75, 'l': 76, 'm': 77, 'n': 78, 'o': 79, 'p': 80, 'q': 81, 'r': 82, 's': 83, 't': 84, 'u': 85, 'v': 86, 'w': 87, 'x': 88, 'y': 89, 'z': 90, '{': 91, '|': 92, '}': 93, '~': 94, '<PADDING>': 95, '<END>': 96}\n"
     ]
    }
   ],
   "source": [
    "print(\"Index : Kannada\")\n",
    "print(index_to_kannada)\n",
    "print(\"\\nKannada : Index\")\n",
    "print(kannada_to_index)\n",
    "print(\"\\n\\nIndex : English\")\n",
    "print(index_to_english)\n",
    "print(\"\\nEnglish : Index\")\n",
    "print(english_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a977ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(english_file, 'r') as file:\n",
    "    english_sentences = file.readlines()\n",
    "with open(kannada_file, 'r') as file:\n",
    "    kannada_sentences = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfed928",
   "metadata": {},
   "source": [
    "# Reading & Processing Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "88550926",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SENTENCES = 100000\n",
    "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
    "kannada_sentences = kannada_sentences[:TOTAL_SENTENCES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6ee006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = [sentence.rstrip('\\n') for sentence in english_sentences]\n",
    "kannada_sentences = [sentence.rstrip('\\n') for sentence in kannada_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d344a48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hes a scientist.', \"'But we speak the truth aur ye sach hai ke Gujarat mein vikas pagal hogaya hai,'' Rahul Gandhi further said in Banaskantha\", '8 lakh crore have been looted.', 'I read a lot into this as well.', \"She was found dead with the phone's battery exploded close to her head the following morning.\", 'How did mankind come under Satans rival sovereignty?', 'And then I became Prime Minister.', 'What about corruption?', 'No differences', '\"\"\"The shooting of the film is 90 percent done.\"']\n"
     ]
    }
   ],
   "source": [
    "print(english_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36055968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.', '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"', 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.', 'ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.', 'ಆಕೆಯ ತಲೆಯ ಹತ್ತಿರ ಇರಿಸಿಕೊಂಡಿದ್ದ ಫೋನ್\\u200cನ ಬ್ಯಾಟರಿ ಸ್ಫೋಟಗೊಂಡು ಆಕೆ ಮೃತಪಟ್ಟಿದ್ದಾಳೆ ಎನ್ನಲಾಗಿದೆ.', 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?', 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.', 'ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?', '‘ಅನುಪಾತದಲ್ಲಿ ವ್ಯತ್ಯಾಸವಿಲ್ಲ’', 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(kannada_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1504cd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(639, 722)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in kannada_sentences), max(len(x) for x in english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afaec199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile length Kannada: 172.0\n",
      "97th percentile length English: 179.0\n"
     ]
    }
   ],
   "source": [
    "PERCENTILE = 97\n",
    "\n",
    "print(f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in kannada_sentences],PERCENTILE)}\")\n",
    "print(f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences],PERCENTILE)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a1bc67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of senteces: 100000\n",
      "Number of valid sentences: 81916\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 200\n",
    "\n",
    "def is_Valid_Tokens(sentence, vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_Valide_Length(sentence, max_sequence_length):\n",
    "    return len(list(sentence)) < (max_sequence_length-1) #need to re-add the <END> token so leaving space for it\n",
    "\n",
    "valid_sentence_indicies = []\n",
    "for index in range(len(kannada_sentences)):\n",
    "    kannada_sentence, english_sentence = kannada_sentences[index], english_sentences[index]\n",
    "    if is_Valide_Length(kannada_sentence, max_sequence_length) and is_Valide_Length(english_sentence, max_sequence_length) and is_Valid_Tokens(kannada_sentence, kannada_vocabulary):\n",
    "        valid_sentence_indicies.append(index)\n",
    "        \n",
    "print(f\"Number of senteces: {len(kannada_sentences)}\")\n",
    "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4375cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kannada_sentences = [kannada_sentences[i] for i in valid_sentence_indicies]\n",
    "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51bc772d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
       "  '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
       "  'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.',\n",
       "  'ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.',\n",
       "  'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?'],\n",
       " ['Hes a scientist.',\n",
       "  \"'But we speak the truth aur ye sach hai ke Gujarat mein vikas pagal hogaya hai,'' Rahul Gandhi further said in Banaskantha\",\n",
       "  '8 lakh crore have been looted.',\n",
       "  'I read a lot into this as well.',\n",
       "  'How did mankind come under Satans rival sovereignty?'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kannada_sentences[:5], english_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03684eb7",
   "metadata": {},
   "source": [
    "# Pytorch Dataset & TextDataset Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "29f873f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, english_sentences, kannada_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.kannada_sentences = kannada_sentences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.kannada_sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "63feb81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(english_sentences, kannada_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6d2d15ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81916"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d0e43137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"'But we speak the truth aur ye sach hai ke Gujarat mein vikas pagal hogaya hai,'' Rahul Gandhi further said in Banaskantha\",\n",
       " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e853a99",
   "metadata": {},
   "source": [
    "# Batching Sentences from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5ddc793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "train_loader = DataLoader(dataset, batch_size)\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d74d70e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hes a scientist.', \"'But we speak the truth aur ye sach hai ke Gujarat mein vikas pagal hogaya hai,'' Rahul Gandhi further said in Banaskantha\", '8 lakh crore have been looted.'), ('ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.', '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"', 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.')]\n",
      "[('I read a lot into this as well.', 'How did mankind come under Satans rival sovereignty?', 'And then I became Prime Minister.'), ('ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.', 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?', 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.')]\n",
      "[('What about corruption?', '\"\"\"The shooting of the film is 90 percent done.\"', 'the Special Statute'), ('ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?', 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.', 'ವಿಶೇಷ ಕಾನೂನು')]\n",
      "[('\"Then the king said to Ittai the Gittite, \"\"Why do you also go with us? Return, and stay with the king. for you are a foreigner, and also an exile. Return to your own place.\"', 'What happened at the UN General Assembly?', 'The meeting was attended by Prime Minister Narendra Modi, Home Minister Amit Shah and Defence Minister Rajnath Singh, among others.'), ('ಆಗ ಅರಸನು ಗಿತ್ತೀಯನಾದ ಇತ್ತೈಯನ್ನು ನೋಡಿ--ನೀನು ನಮ್ಮ ಸಂಗಡ ಬರುವದು ಯಾಕೆ? ನಿನ್ನ ಸ್ಥಳಕ್ಕೆ ಹಿಂದಿರುಗಿ ಹೋಗಿ ಅರಸನ ಸಂಗಡ ಇರು. ಯಾಕಂದರೆ ನೀನು ಸೆರೆಹಿಡಿಯಲ್ಪಟ್ಟವನಾದ ಅನ್ಯದೇಶದವನು.', 'ವಿಶ್ವ ಗೋ ಸಮ್ಮೇಳನದ ಅಂಗಳದಲ್ಲಿ ಏನೇನು ನಡೆದಿದೆ?', 'ಪ್ರಧಾನ ಮಂತ್ರಿ ನರೇಂದ್ರ ಮೋದಿ, ರಕ್ಷಣಾ ಸಚಿವ ರಾಜನಾಥ್ ಸಿಂಗ್ ಮತ್ತು ಕೇಂದ್ರ ಗೃಹ ಸಚಿವ ಅಮಿತ್ ಷಾ ಅವರು ಮಸೂದೆಯ ಬಗ್ಗೆ ಸಾರ್ವಜನಿಕ ಚರ್ಚೆ ಗೆ ಬರುವಂತೆ ಸಂಘ ಸವಾಲು ಹಾಕಿದೆ.')]\n",
      "[('It has been under discussion for a long time.', 'Buses cannot get there.', 'Why then this tradition was not thought of?'), ('ಎಂಬುದು ಬಹಳ ದೀರ್ಘ ಕಾಲದಿಂದಲೂ ಚರ್ಚಿತವಾಗುತ್ತಿರುವ ವಿಷಯ.', 'ಇಲ್ಲಿಗೆ ಬರಲು ಬಸ್ ಸೌಕರ್ಯವೂ ಇಲ್ಲ.', 'ಆ ಪರಂಪರೆ ಯಾಕೆ ಮುನ್ನೆಲೆಗೆ ಬರಲಿಲ್ಲ?')]\n"
     ]
    }
   ],
   "source": [
    "for batch_num, batch in enumerate(iterator):\n",
    "    print(batch)\n",
    "    if batch_num > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec53fa",
   "metadata": {},
   "source": [
    "# Tokenization of Batch Senteces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e5f97",
   "metadata": {},
   "source": [
    "Tokenizing character to number encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "de0affd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, language_to_index, start_token = True, end_token = True):\n",
    "    sentence_word_indicies = [language_to_index[token] for token in list(sentence)]\n",
    "    if start_token:\n",
    "        sentence_word_indicies.insert(0, language_to_index[START_TOKEN])\n",
    "    if end_token:\n",
    "        sentence_word_indicies.append(language_to_index[END_TOKEN])\n",
    "    for _ in range(len(sentence_word_indicies), max_sequence_length):\n",
    "        sentence_word_indicies.append(language_to_index[PADDING_TOKEN])\n",
    "    return torch.tensor(sentence_word_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "47cc711e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It has been under discussion for a long time.',\n",
       "  'Buses cannot get there.',\n",
       "  'Why then this tradition was not thought of?'),\n",
       " ('ಎಂಬುದು ಬಹಳ ದೀರ್ಘ ಕಾಲದಿಂದಲೂ ಚರ್ಚಿತವಾಗುತ್ತಿರುವ ವಿಷಯ.',\n",
       "  'ಇಲ್ಲಿಗೆ ಬರಲು ಬಸ್ ಸೌಕರ್ಯವೂ ಇಲ್ಲ.',\n",
       "  'ಆ ಪರಂಪರೆ ಯಾಕೆ ಮುನ್ನೆಲೆಗೆ ಬರಲಿಲ್ಲ?')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "90552a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenized, kn_tokenized = [], []\n",
    "\n",
    "for sentence_num in range(batch_size):\n",
    "    en_sentence, kn_sentence = batch[0][sentence_num], batch[1][sentence_num]\n",
    "    en_tokenized.append(tokenize(en_sentence, english_to_index, start_token = False, end_token = False))\n",
    "    kn_tokenized.append(tokenize(kn_sentence, kannada_to_index, start_token = True, end_token = True))\n",
    "en_tokenized = torch.stack(en_tokenized)\n",
    "kn_tokenized = torch.stack(kn_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0dcffe55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  50, 111,  78,  96,  73,  96,   1,  78,  90,  85,   1,  73,  95,\n",
       "          82, 106,  59,   1,  56,  93,  84,  73,  94, 111,  73,  84,  97,   1,\n",
       "          61,  82, 106,  61,  94,  71,  86,  93,  58,  96,  71, 106,  71,  94,\n",
       "          82,  96,  86,   1,  86,  94,  88,  81,  15, 124, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123],\n",
       "        [  0,  43,  84, 106,  84,  94,  58, 100,   1,  78,  82,  84,  96,   1,\n",
       "          78,  89, 106,   1,  89, 105,  56,  82, 106,  81,  86,  97,   1,  43,\n",
       "          84, 106,  84,  15, 124, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123],\n",
       "        [  0,  42,   1,  76,  82, 111,  76,  82, 100,   1,  81,  93,  56, 100,\n",
       "           1,  80,  96,  75, 106,  75, 100,  84, 100,  58, 100,   1,  78,  82,\n",
       "          84,  94,  84, 106,  84,  31, 124, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ceeccaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[41, 84,  1, 72, 65, 83,  1, 66, 69, 69, 78,  1, 85, 78, 68, 69, 82,  1,\n",
       "         68, 73, 83, 67, 85, 83, 83, 73, 79, 78,  1, 70, 79, 82,  1, 65,  1, 76,\n",
       "         79, 78, 71,  1, 84, 73, 77, 69, 15, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95],\n",
       "        [34, 85, 83, 69, 83,  1, 67, 65, 78, 78, 79, 84,  1, 71, 69, 84,  1, 84,\n",
       "         72, 69, 82, 69, 15, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95],\n",
       "        [55, 72, 89,  1, 84, 72, 69, 78,  1, 84, 72, 73, 83,  1, 84, 82, 65, 68,\n",
       "         73, 84, 73, 79, 78,  1, 87, 65, 83,  1, 78, 79, 84,  1, 84, 72, 79, 85,\n",
       "         71, 72, 84,  1, 79, 70, 31, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23902be0",
   "metadata": {},
   "source": [
    "#  Masking the Tokenized Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2dc7b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFI = -1e9\n",
    "\n",
    "def create_masks(en_batch, kn_batch):\n",
    "    num_sentences = len(en_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length], True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n",
    "    \n",
    "    for idx in range(num_sentences):\n",
    "        en_sentence_length, kn_sentence_length = len(en_batch[idx]), len(kn_batch[idx])\n",
    "        \n",
    "        en_chars_to_padding_mask = np.arange(en_sentence_length+1, max_sequence_length)\n",
    "        kn_chars_to_padding_mask = np.arange(kn_sentence_length+1, max_sequence_length)\n",
    "        \n",
    "        encoder_padding_mask[idx, :, en_chars_to_padding_mask] = True\n",
    "        encoder_padding_mask[idx, en_chars_to_padding_mask, :] = True\n",
    "        \n",
    "        decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "        \n",
    "        decoder_padding_mask_cross_attention[idx, :, en_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "        \n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFI, 0)\n",
    "    decoder_self_attention_mask = torch.where(look_ahead_mask+decoder_padding_mask_self_attention, NEG_INFI, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFI, 0)\n",
    "    \n",
    "    print(f\"encoder_self_attention_mask: {encoder_self_attention_mask.size()}:{encoder_self_attention_mask[0:10:10]}\")\n",
    "    print(f\"decoder_self_attention_mask: {decoder_self_attention_mask.size()}:{decoder_self_attention_mask[0:10:10]}\")\n",
    "    print(f\"decoder_cross_attention_mask: {decoder_cross_attention_mask.size()}:{decoder_cross_attention_mask[0:10:10]}\")\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "058aa474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_self_attention_mask: torch.Size([3, 200, 200]):tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         ...,\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09]]])\n",
      "decoder_self_attention_mask: torch.Size([3, 200, 200]):tensor([[[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         ...,\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09]]])\n",
      "decoder_cross_attention_mask: torch.Size([3, 200, 200]):tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         ...,\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]]]),\n",
       " tensor([[[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]]]),\n",
       " tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]]]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_masks(batch[0], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ab998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
